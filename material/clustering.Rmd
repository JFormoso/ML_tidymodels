---
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

background-image: url(img/portada.png)
background-size: cover
class: animated slideInRight fadeOutLeft, middle

```{r xaringan-extra-styles, include=FALSE}
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```


```{r include=FALSE}
library(countdown)
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#6c5396",
  secondary_color = "#534173",
  inverse_header_color = "#FFFFFF"
)
```

```{r , message=FALSE, warning=FALSE, include=FALSE} 
library(fontawesome)
library(emo)
library(tidyverse)
```

```{r xaringan-logo, echo=FALSE}
xaringanExtra::use_logo("img/logo-tidymodels.png")
```


```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons()
xaringanExtra::use_fit_screen()
```


# Clustering


### 1º Congreso Latinoamericano de Mujeres en Bioinformática y Ciencia de Datos

---
## Intro

Vamos a usar K-means como un ejemplo útil para aplicar principios de datos ordenados
al análisis estadístico, y especialmente para la distinción entre estas funciones:

* tidy()
* augment()
* glance()
---
# Ejemplo de K-means tomado del blog de tidymodels.org

El origen del siguiente código fue extraído de este 
enlace https://www.tidymodels.org/learn/statistics/k-means/
---
## Creación dataset ejemplo

```{r }
library(tidymodels)
set.seed(27)
centers <- tibble(
  cluster = factor(1:3), 
  num_points = c(100, 150, 50),  # number points in each cluster
  x1 = c(5, 0, -3),              # x1 coordinate of cluster center
  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center
)
labelled_points <- 
  centers %>%
  mutate(
    x1 = map2(num_points, x1, rnorm),
    x2 = map2(num_points, x2, rnorm)
  ) %>% 
  select(-num_points) %>% 
  unnest(cols = c(x1, x2))

ggplot(labelled_points, aes(x1, x2, color = cluster)) +
  geom_point(alpha = 0.3)
```

---
## Uso de kmeans()

Se usa la función kmeans ya incluida en R base.

```{r}
points <- 
  labelled_points %>% 
  select(-cluster)

kclust <- kmeans(points, centers = 3)
kclust
```
---
## Uso de kmeans()

```{r}
summary(kclust)
```
---
## augment()

Ahora a cada datapoint del cluster le agregamos la clasificacion original del dataset

Función augment.kmeans del paquete broom

```{r}
augment(kclust, points)
```

---
### tidy

La funcion tidy nos resume las estadisticas por cluster

```{r}
tidy(kclust)
```
---
### glance

Y la función glance() nos regresa un resumen en una sola fila

```{r}
glance(kclust)
```

---
## Realizar clustering exploratorio

Lo siguiente es para visualizar como cambia la asignacion
de datapoints al cluster, a medida que el número de clusters aumenta.

Es decir, observamos como se agrupan los datapoints a medida 
que k aumenta.
---
###

```{r}
kclusts <- 
  tibble(k = 1:9) %>%
  mutate(
    kclust = map(k, ~kmeans(points, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, points)
  )

kclusts
```
---

```{r}
clusters <- 
  kclusts %>%
  unnest(cols = c(tidied))

assignments <- 
  kclusts %>% 
  unnest(cols = c(augmented))

clusterings <- 
  kclusts %>%
  unnest(cols = c(glanced))
```

---
###

Se grafican los datapoints originales con su cluster predicho / asignado

```{r}
p1 <- 
  ggplot(assignments, aes(x = x1, y = x2)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k)
p1
```

---

Already we get a good sense of the proper number of clusters (3), and how the k-means algorithm functions when k is too high or too low. We can then add the centers of the cluster using the data from tidy():

```{r}
p2 <- p1 + geom_point(data = clusters, size = 10, shape = "x")
p2
```

---

```{r}
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()
```

---

# palmerpenguins

Al primer uso del dataset, debe instalarse el paquete que lo contiene.
Luego podremos cargarlo sin problemas y usarlo para nuestro análisis.

```{r,eval=FALSE}
install.packages("palmerpenguins")
```
---
## Carga del dataset en memoria y breve descripción

```{r}
library(palmerpenguins)
#data(package = 'palmerpenguins')
```

---
Observemos el tipo de sus atributos

```{r}
str(penguins)
```
---
Observamos que los atributos numéricos que pueden ser de interés para kmeans son:

* bill_length_mm
* bill_depth_mm
* flipper_length_mm
* body_mass_g

---
Son datos de pinguinos de tres especies. Observamos
que no se relevó la misma cantidad de datos
para cada especie.

```{r}
penguins %>% 
  count(species)
```
---

Dado que kmeans trabaja con distancias promedios, 
¿nos regresaría algo similar a lo siguiente?
¿tendria sentido el clustering?
```{r}
penguins %>% 
  group_by(species) %>% 
  summarize(across(where(is.numeric), mean, na.rm = TRUE))
```

## Preprocesamiento de datos
---

```{r}
dataset <- penguins %>%
          select(bill_length_mm,bill_depth_mm,body_mass_g,flipper_length_mm)
```
---

```{r}
summary(dataset)
```
---
Hay valores perdidos, procedemos a descartarlos

```{r}
dataset <- dataset %>% 
          drop_na()
```
---
```{r}
summary(dataset)
```

---
```{r}
dataset <- map_df(dataset,as.numeric)
```
---
```{r}
summary(dataset)
```


*TODO histograma plot a esta altura *
---
## Kmeans usando las 4 columnas 

```{r}
kclust <- kmeans(dataset, centers = 3)
kclust
```
---
```{r}
tidy(kclust)
```

---
Ahora escalamos el dataset original

```{r}
kclust <- kmeans(scale(dataset), centers = 3)
tidy(kclust)

```

Rebobinemos, armemos el dataset 