---
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

background-image: url(img/portada.png)
background-size: cover
class: animated slideInRight fadeOutLeft, middle



```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#6c5396",
  secondary_color = "#534173",
  inverse_header_color = "#FFFFFF"
)
```

```{r , message=FALSE, warning=FALSE, include=FALSE} 
library(fontawesome)
library(emo)
library(htmltools)
```

```{r xaringan-extra-styles, include=FALSE}
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```



```{r include=FALSE}
library(countdown)
```


```{r xaringan-logo, echo=FALSE}
xaringanExtra::use_fit_screen()
xaringanExtra::use_logo("img/logo-tidymodels.png")
```


```{r xaringan-tachyons, echo=FALSE}
xaringanExtra::use_tachyons()
xaringanExtra::use_panelset()
```


# Introducción a Machine Learning con `tidymodels`


### 1º Congreso Latinoamericano de Mujeres en Bioinformática y Ciencia de Datos



---

background-image: url(img/equipo-fondo.png)
background-size: cover

## Equipo 

```{r echo=FALSE, out.width = '100%'}
knitr::include_graphics("img/equipo.png")
```
  

---

background-image: url(img/material.png)
background-size: cover

## Material de este tutorial 
<br><br>
## `r emo::ji("earth_americas")` https://ml-tidy-wibds.netlify.app/

<br><br>
## `r fa("github", fill = 'black')` https://bit.ly/ml-wibds 





---

## ¿Qué es modelar? 

.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[
### Es el proceso de desarrollo de una herramienta matemática que genera una predicción precisa. 
### Entrenamos un modelo para encontrar esa predicción precisa. <sup>*</sup>
]



.footnote[<sup>*</sup> Applied Predictive Modeling]
---


## Aprendizaje Automático `r emo::ji("robot")`

.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[
### El aprendizaje automático (ML)-también llamado aprendizaje estadístico- es un subcampo dentro de la inteligencia artificial (IA) donde *los algoritmos "aprenden" patrones en los datos para realizar una tarea específica.* 

]

---

## Conceptos Importantes `r emo::ji("bulb")`

* __Muestra, punto, observación, instancia__ se refiere a una unidad de análisis.
<br>

* __Set de entrenamiento__ son los datos utilizados para el modelado. 
<br>

* __Set de prueba__ son los datos utilizados para medir el desempeño del modelo, entre un conjunto de candidatos. 
<br>

* __Atributos, predictores, variables independientes o descriptores__ son los datos de entrada para la ecuación de predicción.
<br> 
* __Salida, variable dependiente, variable respuesta, clase, o "target"__ es la cantidad a ser predicha. 
<br>

* __Datos categóricos, también conocidos como nominales o atributos__ toman valores específicos que no tienen escala. Ejemplo: bueno/malo, rojo/azul, etc. 
<br>
* __Datos continuos__ son a base de escalas numéricas. El costo de un producto, la presión sanguínea, etc.

.footnote[<sup>*</sup> Applied Predictive Modeling]


---

## Aprendizaje automático e IA


```{r echo=FALSE, out.width = '75%',  fig.align='center'}
knitr::include_graphics("img/ml-ia.png")
```


.footnote[<sup>*</sup> ]

---

## El aprendizaje automático y otros campos relacionados


```{r echo=FALSE, out.width = '90%',  fig.align='center'}
knitr::include_graphics("img/ml-related.png")
```

Fuente: [link](https://ldi.upenn.edu/sites/default/files/Introduction-to-Machine-Learning.pdf)
---

## Aplicaciones en la biología molecular


### [AlphaFold: plegamiento de proteínas](https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery) 


```{r echo=FALSE, out.width = '90%',  fig.align='center'}
knitr::include_graphics("img/prot-fold.png")
```


#### [De novo generation of hit-like molecules from gene expression signatures using artificial intelligence](https://www.nature.com/articles/s41467-019-13807-w)



```{r echo=FALSE, out.width = '90%',  fig.align='center'}
knitr::include_graphics("img/gans.png")
```


---

### Otros hitos en el campo de la IA `r emo::ji("brain")`

        
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/AI-hits.png")
```




---

## Tipos de aprendizaje
        
```{r echo=FALSE, out.width = '90%',  fig.align='center'}
knitr::include_graphics("img/ml.png")
```




---

## Aprendizaje supervisado

```{r echo=FALSE, out.width = '90%',  fig.align='center'}
knitr::include_graphics("img/supervised.png")
```

.footnote[<sup>*</sup> Machine Learning with R, the tidyverse and mlr]

---

## Bias - Variance tradeoff

```{r echo=FALSE, out.width = '90%',  fig.align='center'}
knitr::include_graphics("img/bias-variance.png")
```


*Para una introducción visual ver: http://www.r2d3.us/visual-intro-to-machine-learning-part-2/*


.footnote[<sup>*</sup> Machine Learning with R, the tidyverse and mlr]


---

## Entrenamiento, validación y testeo



```{r echo=FALSE, out.width = '40%',  fig.align='center'}
knitr::include_graphics("img/train-val-test.jpeg")
```


.footnote[Fuente: https://www.tmwr.org/resampling.html]

---
## Resampleo 

El resampleo se utiliza en ML para estimar el desempeño de un modelo. 



```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/resampling.png")
```


.footnote[Fuente: https://www.tmwr.org/resampling.html]

---


## Validación cruzada




```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/CV.png")
```


.footnote[Fuente: https://www.tmwr.org/resampling.html]


---

## Bootstraping 



```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/bootstrap.png")
```





.footnote[Fuente: https://www.tmwr.org/resampling.html]
---


.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[
### Tanto validación cruzada como bootstrap son técnicas de resampleo (resampling).
### La diferencia radica en que la validación cruzada no es con reemplazo mientras que bootstrap sí lo es. 
]




---
# Árboles de decisión

  
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/bret-tree.png")
```


.footnote[Imagen tomada de https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-r-second-edition]

---

## Hiperparámetro



.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[
### Un hiperparámetro es una propiedad de un algoritmo de aprendizaje. Este valor influencia la forma en que trabaja el algoritmo. Estos valores no son aprendidos por el algoritmo desde los datos. Deben ser seteados antes de correr el algoritmo por el analista. 
]


.footnote[The Hundred-page machine learning book. Andriy Burkov]


---

## Hiperparámetros de 
## árboles de decisión

.panelset[
.panel[.panel-name[min_n]

Establezca n mínimo para dividir en cualquier nodo.

Es un método de parada temprana. Se utiliza para evitar el sobreajuste.

]




.panel[.panel-name[tree_depth]

Pone un límite a la profundidad máxima del árbol.

Un método para detener el algoritmo. Se utiliza para evitar el sobreajuste.

]

.panel[.panel-name[cost_complexity]

Agrega un costo o penalización a los errores de árboles más complejos. 

Es una forma de poda. Utilizado para evitar el sobreajuste (overfitting).


]


]

---

# Random Forest

 
```{r echo=FALSE, out.width = '90%',  fig.align='center'}
knitr::include_graphics("img/random-forest1.png")
```


.footnote[Imagen tomada de https://conf20-intro-ml.netlify.com/]

---

# Bootstraping

 
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/random-forest2.png")
```

.footnote[Imagen tomada de https://conf20-intro-ml.netlify.com/]


---

# Bagging
 
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/random-forest3.png")
```


.footnote[Imagen tomada de]

---
### Veamos que sucede al clasificar el conjunto de testeo
 
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/random-forest4.png")
```

.footnote[Imagen tomada de https://conf20-intro-ml.netlify.com/]

---
### Voto mayoritario (majority vote)
 
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/random-forest5.png")
```

.footnote[Imagen tomada de https://conf20-intro-ml.netlify.com/]


---

### Hiperparámetros de 
### random forest

### mtry
* El número de predictores a muestrearse en cada split de árbol

--

### min_n
* el número de observaciones necesarias para seguir dividiendo nodos

  
---
# Métricas `r emo::ji("triangular_ruler")`


.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[

### Durante el modelado de datos es probable que no hagamos un solo modelo, sino varios. 
### La manera de saber qué tan buenos son, es evaluar esos algoritmos mediante métricas.
### Tenemos métricas de regresión y clasificación.
]


---

## Métricas en la clasificación

     
  
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/metricas1.png")
```

---

# Positivos verdaderos
     
  
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/metricas2.png")
```


---

## Falsos positivos 
  
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/metricas3.png")
```


---

# Negativos verdaderos
  
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/metricas4.png")
```

---

## Falsos Negativos  
  
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/metricas5.png")
```



---

## Matriz de Confusión
      
  
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/matriz-confusion.png")
```

    


.footnote[Traducido de 10.7717/peerj.5666/fig-2]




---

## Principios del aprendizaje

### 1. Navaja de Occam `r emo::ji("hocho")`


.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[

### El modelo más simple que pueda ser ajustado es el más factible.

#### Si tuviéramos para elegir entre un modelo simple y uno complejo con el mismo desempeño, debo preferir **siempre** el modelo más simple
]




.footnote[__Learning from data__ Abu-Mostafa, Y.]

---

## Principios del aprendizaje

### 2. Bias en el muestreo (sampling bias)


.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[

### Si los datos contienen un sesgo, el algoritmo tendrá ese mismo sesgo.
]


.footnote[__Learning from data__ Abu-Mostafa, Y.]

---

## Principios del aprendizaje

### 3. Data snooping  `r emo::ji("eyes")`

.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[

### El set de datos de testeo no debe usarse nunca durante la fase de entrenamiento, ya que esto arrojará resultados más optimistas que los esperados. 
]

.footnote[__Learning from data__ Abu-Mostafa, Y.]





---
## De caret a `tidymodels`



.pull-left[
El objetivo de caret era **unificar la sintaxis** para modelizar datos usando como base distintas librerías de R. 
<img src="img/caret.png" width="100%" align="right" />
]

--

.pull-right[
El objetivo de Tidymodels es además hacerlo **en un formato ordenado**. 
<img src="img/tidymodels.png" width="80%" align="right" />

]


---
## `tidymodels`

.bg-near-white.b--purple.ba.bw2.br3.shadow-5.ph4.mt5[

### `tidymodels` es un grupo de paquetes centrado en las tareas de modelización de datos. 
### La modelización consta de varios pasos, la idea es que cada paso lo realice una librería diferente. 
]



.footnote[Sitio web: https://www.tidymodels.org/]

---

# Etapas del modelado de datos


  
```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/modeling.png")
```

.footnote[Sitio web: https://www.tmwr.org/software-modeling.html]


---

## Modelado y `tidymodels`


```{r echo=FALSE, out.width = '100%',  fig.align='center'}
knitr::include_graphics("img/tidy-w.png")
```

---

## Librerías de `tidymodels`


.left-column[
### `library(rsample)`
__Librería que nos permite hacer división del set de datos__

]

.right-column[

```{r echo=FALSE, out.width = '40%',  fig.align='center'}
knitr::include_graphics("img/rsample.png")
```

]





---

## Librerías de `tidymodels`

.left-column[
#### `library(rsample)`
### `library(recipes)`
__Permite hacer preprocesamiento de los datos__
]

.right-column[

```{r echo=FALSE, out.width = '40%',  fig.align='center'}
knitr::include_graphics("img/recipes.png")
```


]

---
## Librerías de `tidymodels`

.left-column[
#### `library(rsample)`
#### `library(recipes)`
### `library(parsnip)`
__Permite unificar los modelos a optimizar__
]

.right-column[

```{r echo=FALSE, out.width = '40%',  fig.align='center'}
knitr::include_graphics("img/parsnip.png")
```


]


---
## Librerías de `tidymodels`

.left-column[
#### `library(rsample)`
#### `library(recipes)`
#### `library(parsnip)`
### `library(workflows)`
__Nos permite unificar el flujo de trabajo__

]

.right-column[

```{r echo=FALSE, out.width = '40%',  fig.align='center'}
knitr::include_graphics("img/workflow.png")
```


]


---
## Librerías de `tidymodels`

.left-column[
#### `library(rsample)`
#### `library(recipes)`
#### `library(parsnip)`
#### `library(workflows)`
### `library(tune)`
__Permite el tuneo de los hiperparámetros de los modelos__
]

.right-column[

```{r echo=FALSE, out.width = '40%',  fig.align='center'}
knitr::include_graphics("img/tune.png")
```


]



---

## Librerías de `tidymodels`

.left-column[
#### `library(rsample)`
#### `library(recipes)`
#### `library(parsnip)`
#### `library(workflows)`
#### `library(tune)`
### `library(yardstick)`
__Permite evaluar las métricas de los modelos__
]

.right-column[

```{r echo=FALSE, out.width = '40%',  fig.align='center'}
knitr::include_graphics("img/yardstick.png")
```


]



---

background-image: url(img/final-fondo.png)
background-size: cover
class: middle

# Muchas gracias `r emo::ji("party")`


---

background-image: url(img/final-fondo.png)
background-size: cover
class: middle

# Recreo `r emo::ji("tea")`

`r countdown(minutes = 15, seconds = 00)`
